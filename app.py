import os
import re
import requests
from flask import Flask, render_template, request, jsonify
import google.generativeai as genai
from pymongo import MongoClient
from pymongo.server_api import ServerApi

app = Flask(__name__)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
GENERATION_MODEL = "gemini-2.5-flash" 
EMBEDDING_MODEL = "models/text-embedding-004" 

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
MONGO_URI = os.environ.get("MONGO_URI")
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")

DB_NAME = "my_rag_db"
COLLECTION_NAME = "perplex_context"
INDEX_NAME = "vector_index"

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
try:
    if not MONGO_URI:
        print("âŒ FATAL: MONGO_URI is missing.")
        mongo_client = None
        collection = None
    else:
        mongo_client = MongoClient(MONGO_URI, server_api=ServerApi('1'))
        db = mongo_client[DB_NAME]
        collection = db[COLLECTION_NAME]
        # ØªØ³Øª Ø§ØªØµØ§Ù„ Ø³Ø±ÛŒØ¹
        mongo_client.admin.command('ping')
        print(f"âœ… Connected to MongoDB Atlas. DB: {DB_NAME}, Coll: {COLLECTION_NAME}")
except Exception as e:
    print(f"âŒ DB Connection Error: {e}")
    mongo_client = None
    collection = None

# --- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ---

def recursive_chunk_text(text, chunk_size=1000, overlap=100):
    if not text: return []
    text = re.sub(r'\s+', ' ', text).strip()
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        if end >= text_len:
            chunks.append(text[start:])
            break
        
        block = text[start:end]
        split_point = -1
        match = re.search(r'[.!?]\s+', block[::-1])
        if match: split_point = len(block) - match.start()
        
        if split_point == -1:
            last_space = block.rfind(' ')
            if last_space != -1: split_point = last_space
        
        if split_point == -1: split_point = chunk_size
            
        chunks.append(text[start : start + split_point])
        start += split_point - overlap
    
    # DEBUG: Ú†Ø§Ù¾ ØªØ¹Ø¯Ø§Ø¯ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
    print(f"âœ‚ï¸ Chunked text into {len(chunks)} parts.")
    return chunks

def get_embedding(text, task_type="retrieval_document"):
    if not text or not text.strip(): return None
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL,
            content=text,
            task_type=task_type
        )
        emb = result['embedding']
        # DEBUG: Ú†Ø§Ù¾ Ø·ÙˆÙ„ ÙˆÚ©ØªÙˆØ± (ÙÙ‚Ø· ÛŒÚ©Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù†)
        # print(f"ğŸ“ Vector Dimension Generated: {len(emb)}") 
        return emb
    except Exception as e:
        print(f"âš ï¸ Embedding Error: {e}")
        return None

def generate_search_queries(prompt):
    try:
        model = genai.GenerativeModel(GENERATION_MODEL)
        sys_prompt = (
            f"User prompt: '{prompt}'\n"
            "Generate 3 specific search queries. Return ONLY the queries separated by newlines."
        )
        resp = model.generate_content(sys_prompt)
        return [q.strip() for q in resp.text.split('\n') if q.strip()]
    except Exception as e:
        print(f"Query Gen Error: {e}")
        return [prompt]

def tavily_search(queries):
    if not TAVILY_API_KEY: 
        print("âš ï¸ Tavily Key Missing")
        return []
    combined_results = []
    for q in queries[:2]: 
        try:
            resp = requests.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": TAVILY_API_KEY,
                    "query": q,
                    "search_depth": "basic",
                    "max_results": 3
                }
            )
            data = resp.json()
            if 'results' in data:
                combined_results.extend(data['results'])
        except Exception as e:
            print(f"Tavily Error: {e}")
    return combined_results

# --- Ù…Ø³ÛŒØ±Ù‡Ø§ ---

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/run_agent', methods=['POST'])
def run_agent():
    if collection is None:
        return jsonify({"error": "Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"}), 500

    # 1. Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§ÛŒ Ù‚Ø¨Ù„ÛŒ
    try: 
        collection.delete_many({}) 
        print("ğŸ§¹ Database Cleared for new session.")
    except Exception as e:
        print(f"Delete Error: {e}")

    prompt = request.form.get('prompt')
    file = request.files.get('file')
    
    if not prompt: return jsonify({"error": "Ø³ÙˆØ§Ù„ ÙˆØ§Ø±Ø¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"}), 400

    response_data = {"generated_queries": [], "all_sources": [], "retrieved_chunks": [], "answer": "", "logs": []}
    
    # Ù…ØªØºÛŒØ± Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ² ÙˆÚ©ØªÙˆØ±
    debug_vec_dim = 0

    # 2. Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„
    if file and file.filename != '':
        response_data["logs"].append("ğŸ“‚ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡...")
        try:
            text = file.read().decode('utf-8')
            chunks = recursive_chunk_text(text)
            docs = []
            for ch in chunks:
                emb = get_embedding(ch, task_type="retrieval_document")
                if emb:
                    if debug_vec_dim == 0: debug_vec_dim = len(emb)
                    docs.append({"text": ch, "embedding": emb, "source": "File: " + file.filename})
            if docs:
                collection.insert_many(docs)
                msg = f"âœ… {len(docs)} Ø¨Ø®Ø´ Ø§Ø² ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. (Vector Dim: {debug_vec_dim})"
                print(msg)
                response_data["logs"].append(msg)
                response_data["all_sources"].append({"title": file.filename, "url": "#", "type": "file"})
        except Exception as e:
            response_data["logs"].append(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ§ÛŒÙ„: {str(e)}")

    # 3. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙˆØ¨
    response_data["logs"].append("ğŸŒ ØªÙˆÙ„ÛŒØ¯ Ú©ÙˆØ¦Ø±ÛŒ Ùˆ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙˆØ¨...")
    queries = generate_search_queries(prompt)
    response_data["generated_queries"] = queries
    
    search_results = tavily_search(queries)
    
    if search_results:
        web_docs = []
        seen_urls = set()
        for res in search_results:
            if res['url'] not in seen_urls:
                response_data["all_sources"].append({"title": res['title'], "url": res['url'], "type": "web"})
                seen_urls.add(res['url'])
            
            content = res.get('content', '')
            if len(content) < 50: continue # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ù…Ø­ØªÙˆØ§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡

            web_chunks = recursive_chunk_text(content, chunk_size=800)
            for ch in web_chunks:
                emb = get_embedding(ch, task_type="retrieval_document")
                if emb:
                    if debug_vec_dim == 0: debug_vec_dim = len(emb)
                    web_docs.append({
                        "text": ch, 
                        "embedding": emb, 
                        "source": res.get('url'),
                        "title": res.get('title')
                    })
        if web_docs:
            collection.insert_many(web_docs)
            msg = f"ğŸŒ {len(web_docs)} ØªÚ©Ù‡ Ø¯Ø§Ù†Ø´ Ø§Ø² ÙˆØ¨ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. (Vector Dim: {debug_vec_dim})"
            print(msg)
            response_data["logs"].append(msg)
    
    # --- 4. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (Ø¨Ø®Ø´ Ø­ÛŒØ§ØªÛŒ Ø¯ÛŒØ¨Ø§Ú¯) ---
    response_data["logs"].append("ğŸ¤” Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·...")
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ú©ÛŒÙˆÙ…Ù†Øªâ€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² Ø³Ø±Ú†
    total_docs = collection.count_documents({})
    print(f"ğŸ“Š DB STATUS: Total Documents in DB: {total_docs}")
    response_data["logs"].append(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {total_docs}")

    query_emb = get_embedding(prompt, task_type="retrieval_query")
    
    retrieved = []
    if query_emb:
        print(f"â“ Query Vector Dimension: {len(query_emb)}")
        
        # Ù‡Ø´Ø¯Ø§Ø± Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø§Ø¨Ø¹Ø§Ø¯
        if debug_vec_dim > 0 and len(query_emb) != debug_vec_dim:
             print("ğŸš¨ CRITICAL ERROR: Dimension Mismatch!")
             response_data["logs"].append(f"ğŸš¨ Ø®Ø·Ø§ÛŒ Ø§Ø¨Ø¹Ø§Ø¯: Ø¯ÛŒØªØ§Ø¨ÛŒØ³={debug_vec_dim} ÙˆÙ„ÛŒ Ú©ÙˆØ¦Ø±ÛŒ={len(query_emb)}")

        pipeline = [
            {
                "$vectorSearch": {
                    "index": INDEX_NAME,
                    "path": "embedding",
                    "queryVector": query_emb,
                    "numCandidates": 100, # Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ù…Ù†Ù‡
                    "limit": 10
                }
            },
            {"$project": {"_id": 0, "text": 1, "source": 1, "title": 1, "score": {"$meta": "vectorSearchScore"}}}
        ]
        
        try:
            retrieved = list(collection.aggregate(pipeline))
            print(f"ğŸ¯ Vector Search Results: {len(retrieved)}")
            response_data["retrieved_chunks"] = retrieved
        except Exception as e:
            print(f"âŒ Aggregation Error: {e}")
            response_data["logs"].append(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±: {e}")

    # 5. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
    response_data["logs"].append("âœï¸ Ù†Ú¯Ø§Ø±Ø´ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ...")
    context_text = ""
    
    # Ø§Ú¯Ø± ÙˆÚ©ØªÙˆØ± Ø³Ø±Ú† Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ØŒ Ø­Ø¯Ø§Ù‚Ù„ Ù…ØªÙ† Ø®Ø§Ù… ÙˆØ¨ Ø±Ø§ Ø¨Ø¯Ù‡ÛŒÙ… (Fail-safe)
    if not retrieved:
        print("âš ï¸ Vector search returned 0 results. Using raw fallback.")
        response_data["logs"].append("âš ï¸ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ù†ØªÛŒØ¬Ù‡ Ù†Ø¯Ø§Ø´Øª. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªÙ† Ø®Ø§Ù….")
        if search_results:
            for res in search_results:
                context_text += f"Source ({res['title']}): {res.get('content', '')[:600]}...\n\n"
        else:
            context_text = "No context found."
    else:
        for doc in retrieved:
            src = doc.get('title', doc.get('source'))
            context_text += f"Source ({src}): {doc['text']}\n\n"

    try:
        final_model = genai.GenerativeModel(GENERATION_MODEL)
        final_prompt = (
            f"User Question: {prompt}\n\n"
            f"Based ONLY on the following context, write a comprehensive answer in Persian (Farsi).\n"
            f"Cite sources inline like [Source Name].\n"
            f"CONTEXT:\n{context_text}"
        )
        answer_resp = final_model.generate_content(final_prompt)
        response_data["answer"] = answer_resp.text
    except Exception as e:
        response_data["answer"] = f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {str(e)}"

    return jsonify(response_data)

if __name__ == '__main__':
    app.run(debug=True)
