import os
import time
import requests
import re
from flask import Flask, render_template, request, jsonify
import google.generativeai as genai
from pymongo import MongoClient
from pymongo.server_api import ServerApi

app = Flask(__name__)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
MONGO_URI = os.environ.get("MONGO_URI")
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")

DB_NAME = "my_rag_db"
COLLECTION_NAME = "perplex_context" # Ú©Ø§Ù„Ú©Ø´Ù† Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¹Ø¯Ù… ØªØ¯Ø§Ø®Ù„ Ø¨Ø§ Ù‚Ø¨Ù„ÛŒ
INDEX_NAME = "vector_index"

genai.configure(api_key=GEMINI_API_KEY)

# Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
try:
    mongo_client = MongoClient(MONGO_URI, server_api=ServerApi('1'))
    db = mongo_client[DB_NAME]
    collection = db[COLLECTION_NAME]
    print("âœ… Connected to MongoDB")
except Exception as e:
    print(f"âŒ DB Connection Error: {e}")

# --- 1. Ú†Ø§Ù†Ú©â€ŒØ¨Ù†Ø¯ÛŒ Ø§ØµÙˆÙ„ÛŒ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ (Recursive with Overlap) ---
def recursive_chunk_text(text, chunk_size=800, overlap=100):
    """
    Ù…ØªÙ† Ø±Ø§ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
    1. Ø§ÙˆÙ„ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø§ Ø¯Ùˆ Ø§ÛŒÙ†ØªØ± (Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù) Ø¬Ø¯Ø§ Ú©Ù†Ø¯.
    2. Ø§Ú¯Ø± Ù†Ø´Ø¯ØŒ Ø¨Ø§ ÛŒÚ© Ø§ÛŒÙ†ØªØ±.
    3. Ø§Ú¯Ø± Ù†Ø´Ø¯ØŒ Ø¨Ø§ Ù†Ù‚Ø·Ù‡ (Ù¾Ø§ÛŒØ§Ù† Ø¬Ù…Ù„Ù‡).
    4. Ù†Ù‡Ø§ÛŒØªØ§ Ø¨Ø§ ÙØ§ØµÙ„Ù‡ (Ú©Ù„Ù…Ø§Øª).
    Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ (Overlap) Ø¯Ø§Ø±Ø¯ ØªØ§ Ù…Ø¹Ù†ÛŒ Ø¯Ø± Ù…Ø±Ø² Ø¨Ø±Ø´â€ŒÙ‡Ø§ Ú¯Ù… Ù†Ø´ÙˆØ¯.
    """
    if not text: return []
    
    # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø§ÙˆÙ„ÛŒÙ‡ Ù…ØªÙ†
    text = re.sub(r'\s+', ' ', text).strip()
    
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        
        if end >= text_len:
            chunks.append(text[start:])
            break
            
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø¨Ø±Ø´ (Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª)
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø´ Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø¬Ù…Ù„Ù‡ (Ù†Ù‚Ø·Ù‡ ÛŒØ§ Ø¹Ù„Ø§Ù…Øª Ø³ÙˆØ§Ù„/ØªØ¹Ø¬Ø¨)
        block = text[start:end]
        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ù†ÛŒÙ…Ù‡ Ø¯ÙˆÙ… Ú†Ø§Ù†Ú© (ØªØ§ Ú†Ø§Ù†Ú© Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú© Ù†Ø´ÙˆØ¯)
        split_point = -1
        
        # Ø§ÙˆÙ„ÙˆÛŒØª Û±: Ù¾Ø§ÛŒØ§Ù† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù ÛŒØ§ Ø¬Ù…Ù„Ù‡
        match = re.search(r'[.!?]\s+', block[::-1]) # Ø¬Ø³ØªØ¬Ùˆ Ø§Ø² Ø¢Ø®Ø± Ø¨Ù‡ Ø§ÙˆÙ„
        if match:
            split_point = len(block) - match.start()
        
        # Ø§ÙˆÙ„ÙˆÛŒØª Û²: Ø§Ú¯Ø± Ù†Ù‚Ø·Ù‡ Ù†Ø¨ÙˆØ¯ØŒ ÙØ§ØµÙ„Ù‡ (Space)
        if split_point == -1:
            last_space = block.rfind(' ')
            if last_space != -1:
                split_point = last_space
        
        # Ø§Ú¯Ø± Ù‡ÛŒÚ†Ú©Ø¯Ø§Ù… Ù†Ø¨ÙˆØ¯ (ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ)ØŒ Ø¨Ø±Ø´ Ø³Ø®Øª
        if split_point == -1:
            split_point = chunk_size
            
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú†Ø§Ù†Ú©
        final_chunk = text[start : start + split_point]
        chunks.append(final_chunk)
        
        # Ø­Ø±Ú©Øª Ø¨Ù‡ Ø¬Ù„Ùˆ (Ø¨Ø§ Ú©Ø³Ø± Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ú©Ø§Ù†ØªÚ©Ø³Øª)
        start += split_point - overlap
        
    return chunks

# --- 2. Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ ---
def get_embedding(text):
    try:
        result = genai.embed_content(
            model="models/text-embedding-005",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']
    except:
        return None

def generate_search_queries(prompt):
    """ØªÙˆÙ„ÛŒØ¯ Ú©ÙˆØ¦Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ø¬Ù…ÛŒÙ†Ø§ÛŒ"""
    model = genai.GenerativeModel('gemini-2.5-flash')
    sys_prompt = (
        f"User prompt: '{prompt}'\n"
        "Generate 3 specific, effective search queries to find information about this prompt on Google. "
        "Return ONLY the queries separated by newlines. Do not number them."
    )
    resp = model.generate_content(sys_prompt)
    return [q.strip() for q in resp.text.split('\n') if q.strip()]

def tavily_search(queries):
    """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙˆØ¨ Ø¨Ø§ Tavily"""
    combined_results = []
    # ÙÙ‚Ø· Ú©ÙˆØ¦Ø±ÛŒ Ø§ÙˆÙ„ Ùˆ Ø¯ÙˆÙ… Ø±Ø§ Ø¬Ø³ØªØ¬Ùˆ Ù…ÛŒÚ©Ù†ÛŒÙ… ØªØ§ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ Ø¨Ù…Ø§Ù†Ø¯
    for q in queries[:2]: 
        try:
            resp = requests.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": TAVILY_API_KEY,
                    "query": q,
                    "search_depth": "basic",
                    "include_answer": False,
                    "include_raw_content": False,
                    "max_results": 3
                }
            )
            data = resp.json()
            if 'results' in data:
                combined_results.extend(data['results'])
        except Exception as e:
            print(f"Tavily Error: {e}")
    return combined_results

# --- Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø³Ø§ÛŒØª ---
@app.route('/')
def home():
    return render_template('index.html')

@app.route('/run_agent', methods=['POST'])
def run_agent():
    # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¨Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù‡Ø± Ø¨Ø§Ø± ÛŒÚ© Ø³Ø±Ú† Ø¬Ø¯ÛŒØ¯ Ø¨Ø§Ø´Ø¯)
    # Ø¯Ø± Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ÛŒØ¯ Session ID Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…ØŒ ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø®ØµÛŒ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø§ÙˆÚ©ÛŒÙ‡
    try:
        collection.delete_many({}) 
    except: pass

    prompt = request.form.get('prompt')
    file = request.files.get('file')
    
    if not prompt:
        return jsonify({"error": "Ù„Ø·ÙØ§ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯"}), 400

    steps = [] # Ù„Ø§Ú¯ Ù…Ø±Ø§Ø­Ù„ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±
    
    # --- Ù…Ø±Ø­Ù„Ù‡ Û±: Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ (Ø§Ú¯Ø± Ø¨Ø§Ø´Ø¯) ---
    if file and file.filename != '':
        steps.append("ğŸ“‚ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡...")
        text = file.read().decode('utf-8')
        chunks = recursive_chunk_text(text)
        docs = []
        for ch in chunks:
            emb = get_embedding(ch)
            if emb:
                docs.append({"text": ch, "embedding": emb, "source": "File: " + file.filename})
        if docs:
            collection.insert_many(docs)
            steps.append(f"âœ… {len(docs)} Ø¨Ø®Ø´ Ø§Ø² ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

    # --- Ù…Ø±Ø­Ù„Ù‡ Û²: ØªÙˆÙ„ÛŒØ¯ Ú©ÙˆØ¦Ø±ÛŒ Ùˆ Ø¬Ø³ØªØ¬Ùˆ ---
    steps.append("ğŸŒ Ø¯Ø± Ø­Ø§Ù„ Ø·Ø±Ø§Ø­ÛŒ Ú©ÙˆØ¦Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ...")
    queries = generate_search_queries(prompt)
    steps.append(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙˆØ¨ Ø¨Ø±Ø§ÛŒ: {queries}")
    
    search_results = tavily_search(queries)
    
    # --- Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬ ÙˆØ¨ ---
    steps.append(f"ğŸŒ {len(search_results)} ØµÙØ­Ù‡ ÙˆØ¨ Ù¾ÛŒØ¯Ø§ Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ú†Ø§Ù†Ú©â€ŒØ¨Ù†Ø¯ÛŒ...")
    web_docs = []
    for res in search_results:
        # Ù…Ø­ØªÙˆØ§ÛŒ ÙˆØ¨ Ø±Ø§ Ù‡Ù… Ú†Ø§Ù†Ú© Ù…ÛŒÚ©Ù†ÛŒÙ…
        content = res.get('content', '')
        web_chunks = recursive_chunk_text(content, chunk_size=800, overlap=100)
        for ch in web_chunks:
            emb = get_embedding(ch)
            if emb:
                web_docs.append({
                    "text": ch, 
                    "embedding": emb, 
                    "source": res.get('url'),
                    "title": res.get('title')
                })
    
    if web_docs:
        collection.insert_many(web_docs)
        steps.append(f"ğŸ§  {len(web_docs)} Ø¨Ø®Ø´ Ø¯Ø§Ù†Ø´ Ø§Ø² ÙˆØ¨ Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")

    # --- Ù…Ø±Ø­Ù„Ù‡ Û´: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (Retrieval) ---
    steps.append("ğŸ¤” Ø¯Ø± Ø­Ø§Ù„ ÙÚ©Ø± Ú©Ø±Ø¯Ù† Ùˆ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª...")
    
    # ÙˆÚ©ØªÙˆØ± Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ
    query_emb = get_embedding(prompt)
    
    pipeline = [
        {
            "$vectorSearch": {
                "index": INDEX_NAME,
                "path": "embedding",
                "queryVector": query_emb,
                "numCandidates": 100,
                "limit": 8 # 8 ØªÚ©Ù‡ Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±
            }
        },
        {"$project": {"_id": 0, "text": 1, "source": 1, "title": 1}}
    ]
    retrieved = list(collection.aggregate(pipeline))
    
    # --- Ù…Ø±Ø­Ù„Ù‡ Ûµ: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ ---
    context_text = ""
    sources_list = set()
    
    for doc in retrieved:
        source_info = doc.get('title', doc.get('source'))
        context_text += f"Source ({source_info}): {doc['text']}\n\n"
        sources_list.add(doc.get('source'))

    final_model = genai.GenerativeModel('gemini-2.5-flash')
    final_prompt = (
        f"You are an AI research assistant (like Perplexity). \n"
        f"User Question: {prompt}\n\n"
        f"Based ONLY on the following context, write a comprehensive, well-structured answer. "
        f"Cite your sources using [1], [2] etc. if possible, or explicitly mention the source name.\n"
        f"If the context doesn't answer the question, admit it.\n\n"
        f"CONTEXT:\n{context_text}"
    )
    
    answer_response = final_model.generate_content(final_prompt)
    
    return jsonify({
        "steps": steps,
        "answer": answer_response.text,
        "sources": list(sources_list)
    })

if __name__ == '__main__':
    app.run(debug=True)
